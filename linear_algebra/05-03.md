# Overview, LA 05/03
In this Lecture, we dive deep on matrices and how they can help us, what kinds
of special and named matrices exist, and how specific elements of matrices are
named and used to solve problems. Also, this chapter was a bitch to type.

## Content

### Cont. of Vectors and Matrices
Ist $A = (a_{ij}) \in K^{m \times n}$, dann heisst $\begin{pmatrix}a_{i1} & \dots & a_{in} \end{pmatrix}$ 
**i-te Zeile** und $\begin{pmatrix}a_{1j} \\ \dots \\ a_{nj} \end{pmatrix}$ **j-te Spalte** von $A$. 

Gilt $m = n$ so heisst $A$ **quadratisch**

Die Matrix $A^T$ ist definiert: 
$$A^T := 
\begin{pmatrix}
  a_{11} & a_{12} & \dots & a_{m1} \\ 
  a_{12} & a_{22} & \dots & a_{m2} \\ 
  \vdots & \vdots & \ddots & \vdots \\ 
  a_{1n} & a_{2n} & \dots & a_{mn} 
\end{pmatrix} = (a_{ji}) \in K^{n \times m}$$

Dies ist die **transponierte Matrix** bei der die Indices vertauscht sind.

Ist $A$ quadratisch und gilt $A^T = A$ ($\forall i,j : a_{ij} = a_{ji}$) so heisst $A$ **symmetrisch**.

### 3.2 Bsp/Def: 
a. Seien $S_1, \dots, S_n$ Staedte und $d_{ij}$ die Entfernung zwischen $S_i$ und $S_j$.  
   Dann heisst $D = (d_{ij}) \in R^{m \times n}$ **Distanzmatrix**. Sind diese Entfernungen "luftlinie"
   so gilt $d_{ij} = d_{ji}$, also D _symmetrisch_. Messen wir die Entferning als Reisezeiten, 
   z.B. Fluss auf/abwaerts so kann natuerlich $d_{ij} \neq d_{ji}$ gelten. 
b. Sei $G = (V,E)$ ein Graph mit $V = \{v_1, \dots, v_n \}$ und $E = \{ e_1, \dots, e_m \}$.
   Dann heisst die Matrix $$A = (a_{ij}) \in GF_2^{n\times n} \text{ mit } a_{ij} = \begin{cases}1, & \{v_i, v_j\} \in E \\ 0, & else \end{cases}$$
   **Adjazenzmatrix**  von $G$. (Ungerichteter Graph $\to$ symmetrische Matrix $A$)  
   Und die Matrix $$B = (b_{ij}) \in GF_2^{n\times m} \text{ mit } b_{ij} = \begin{cases}1, & v_i \in e_j \\ 0, & else \end{cases}$$
   **Inzidenzmatrix** von $G$.
c. Sei $G = (V,E)$ ein Digraph mit $V= \{v_1, \dots, v_n\}$ und $E = \{e_1, \dots, e_m\}$.  
   Dann heisst die Matrix 
   $$C=(c_{ij}) \in GF_3^{n \times m} \text{ mit } c_{ij} = \begin{cases}-1, & e_j = (v_i, w) \text{ mit }w \in V \\ 1, & e_j = (v, v_i) \text{ mit }v \in V \\ -, & else  \end{cases} $$ 
   wiederum **Inzidenzmatrix** von $G$ im gerichteten Graphen. 

Eine kurze Zusammenfassung der Matrixarten die benannt/unterschieden werden: 

- Eine **Distanzmatrix** zeigt die Distanz in einem (gewichteten) Graphen an. Wenn der Weg $a \to b$
  mit 5 gewichtet ist, so ist $D_{(a,b)} = 5$. Ein ungerichteter Graph ist _symmetrisch_. Die 
  Distanzmatrix wird oft mit $D$ repraesentiert. 
- Eine **Adjazenzmatrix** zeigt die Verbindungen in einem Graph, so wuerde eine Verbindung $t \to v$
  zu $A_{(tv)} = 1$ fuehren . Hierbei zaehlt ein Loop $a\to a$ als 2 Verbindungen. Ein ungerichteter
  Graph ist ebenfalls _symmetrisch_. Die Adjazenzmatrix wird oft mit $A$ repraesentiert.
- Die **Inzidenzmatrix** von $G$ zeigt ebenfalls die Verbindungen in einem Graphen, jedoch
  nicht durch durch eine $e \times e$ matrix, sondern durch eine $e \times v$ matrix, wo pro
  Kante (Spalte) zwei Knoten (Reihen) auf $1$ gesetzt werden, wo die Knoten "greifen".
    - Die **Inzidenzmatrix fuer gerichtete Graphen** hat anstatt zwei Einsen eine $1$ wo die Kante
      den Knoten "verlaesst", und eine $-1$ wo die Kante "ankommt".

*Matrizen sind ein **effektives** Format zur Speicherung von Daten und Graphen.*

### 3.3 Def: Matrix Operationen
a. Seien $A = (a_{i,j}), B = (b_{ij}) \in K^{m\times n}$
   Dann ist die **Summe** $A+B := (a_{ij} + b_{ij}) \in K^{m\times n}$  
   Komponentenweise Addition: Assoziativ/Kommutativgesetze erfuellt,  
   Nullmatrix = Neutrales Element  
   $-A = (-a_{ij})_{ij}$ = inverses Element.  
   $\Rightarrow (K^{m \times n}, +)$ ist eine **additive, kommutative Gruppe**
b. Seien $A = (a_{ik}) \in K^{m \times n}, B = (B_{kj}) \in K^{n \times l}$.   
   Dann ist das **Produkt** in $A \cdot B = (c_{ij}) \in K^{m \times l}$ definiert durch  
   $$c_{ij} = \sum^n_{k=1}a_{ik}b_{kj}$$ ("Zeile mal Spalte", nicht komponentenweise.)  
   *CAUTION!: Das Produkt ist nur dann definiert wenn die Spaltenzahl von $A$ mit der 
   Zeilenzahl von $B$ uebereinstimmt!*  
   Wichtiger Spezialfall: $$A = (a_{ik}) \in K^{m\times n}, v = \begin{pmatrix}v_1 \\ \vdots \\ v_n \end{pmatrix} = \begin{pmatrix}v_1 & \dots & v_n \end{pmatrix}^T \in K^n (= K^{1\times n})$$
   $$ A \cdot v = \begin{pmatrix}w_1 & \dots & w_m\end{pmatrix}^T \text{ mit } w_i = \sum_{k=1} a_{ik}v_k$$
c. Sei $A = (a_{ij}) \in K^{m \times n}, \lambda \in K$  
   Dann ist das Produkt mit einem Skalar $\lambda \cdot A := (\lambda \cdot a_{ij}) \in K^{m \times n}$
   komponentenweise definiert.

### 3.4 Beispiele
$$\begin{pmatrix}1 & 2 \\ 3 & 4 \end{pmatrix} + \begin{pmatrix}5 & 6 \\ 7 & 8 \end{pmatrix} = \begin{pmatrix}6 & 8 \\ 10 & 12 \end{pmatrix}$$
$$2 \cdot \begin{pmatrix} 1 & 3 \\ 5 & 7 \end{pmatrix} = \begin{pmatrix}2 & 6 \\ 10 & 14 \end{pmatrix}$$
$$\begin{pmatrix}1 & 1 & 1 \\ -1 & 1 & -1 \end{pmatrix} \cdot \begin{pmatrix}1 & 2 \\ 2 & 1 \\ 1 & 2 \end{pmatrix}  = \begin{pmatrix}4 & 5 \\ 0 & -3  \end{pmatrix}$$
aber: 

$$ \begin{pmatrix}1 & 2 \\ 2 & 1 \\ 1 & 2 \end{pmatrix}\begin{pmatrix}1 & 1 & 1 \\ -1 & 1 & -1\end{pmatrix} = \begin{pmatrix} -1 & 3 & -1 \\ 1 & 3 & 1 \\ -1 & 3 & -1 \end{pmatrix}$$

*Das Produkt ist **nicht kommutativ**, auch wenn beide Produkte existieren.*

$$v = \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix}, w = \begin{pmatrix} w_1 \\ \vdots \\ w_n \end{pmatrix} \in K^{n} = K{n \times 1}$$
$$ v^T =\begin{pmatrix} v_1& \dots & v_n\end{pmatrix} \in K^{1 \times n} \text { und}$$
$$ v^T w = \sum^n_{i=1}v_i w_i$$ ist das sog. **Skalarprodukt** von $v,w$, sodass ein Skalar am Ende rauskommt. 

Dieses ist eingentlich eine $1 \times 1$ Matrix, wird aber als Skalar behandelt.

### 3.5 Lemma
a. $(K^{m \times n}, +)$ ist eine kommutative Gruppe
b. Seien $A,B \in K^{m\times n}$ und $\lambda, \mu \in K$. Dann gilt: 
    - $\lambda \cdot (A+B) = \lambda \cdot A + \lambda \cdot B$
    - $(\lambda + \mu) \cdot A = \lambda \cdot A + \mu \cdot A$
    - $\lambda \cdot (\mu \cdot A) = (\lambda \cdot \mu) \cdot A$
    - $1 \cdot A = A$, mit 1 als neutrales Element von K.
c. Seien $A,B,C$ Matrizen passender Dimensionen.  
   Dann gilt: 
    1. $(A\cdot B) \cdot C = A \cdot (B \cdot C)$
    2. $A \cdot (B + C) = A \cdot B + A\cdot C$
    3. $(A + B) \cdot C = A \cdot C + B\cdot C$

### 3.6 Def/Lemma
Sei 
$$I_n = \begin{pmatrix}1 & 0 & \dots & 0 \\ 0 & 1 & \dots & 0 \\ \vdots & \vdots & \ddots& \vdots\\ 0 & 0 & \dots & 1 \end{pmatrix} \in K^{n\times n}$$
Dann heisst $I_n$ **Einheitsmatrix** und es gilt: 
$$A \in K^{m \times n}, B \in K^{n\times l}: A \cdot I_n = A, I_n \cdot B = B$$ 

Der erste Anwendungsbereich fuer MAtrizen ist die Loesung **Linerer Gleichungsysteme (LGS)**

### Beispiel
Gegeben sei ein Lineares Gleichungssystem. Wir fassen die Koeffizienten an den 
Variablen in einer Matrix A zusammen und die rechte Seite in einem Vektor B, also: 
$$ A = \begin{pmatrix} 1 & 0 & 2 & 1 \\ 2 & 0 & 4 & -2 \\ 0 & 1 & 0 & -1 \\ 1 & 0 & 2 & 2 \end{pmatrix}, b = \begin{pmatrix}-3 \\ 2 \\ 2 \\ -5\end{pmatrix}$$

Aufgrund der Def. der Matrix Multiplikation gilt nun fuer den Vektor $x = (x_1, \dots, x_4)^T$, dass: 
$$ A \cdot x = \begin{pmatrix}1 & 0 & 2 & 1 \\ 2 & 0 & 4 & -2 \\ 0 & 1 & 0 & -1 \\ 1 & 0 & 2 & 2 \end{pmatrix} \cdot x = b$$

Unser LGS kann also als $A \cdot x = b$ zusammengefasst werden. 


## Glossary
- $GF_2$ bezieht sich auf den Koerper $GF_p = ([p-1]_0,+,\cdot)$ vom letzten Kapitel.
- _Digraph_ = Directed Graph

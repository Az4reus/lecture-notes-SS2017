# Overview, GAD 05/02
In this lecture, 

## Content
In der Effizienzmessung gibt es drei normal relevante Faelle: 

- Worst Case: Liefert eine _Garantie_, aber evtl. sehr pessimistisch. 
- Average Case: Nuetzliche Verallgemeinerung der Realitaet. 
- Best Case: Evtl sehr optimistisch, aber kann im Vergleich mit worst-case
  eine Idee ueber die moegliche Abweichung geben. 

Generell sind exakte Werte fuer $T(n)$ sehr aufwendig und es nicht wert. 
Man betrachtet deswegen normalerweise das **asymptotische Wachstum des Algorithmus..** 
($n \to \infty$)


### Wachstumsrate/ordnung
$f(n)$ and $g(n)$ haben die **gleiche** Wachstumsrate, falls das Verhaeltnis durch 
Konstanten beschraenkt ist: 
$$ \exists c,d \in \mathbb{R}_+ : \exists n_0 \in \mathbb{N} : \forall n \geq n_0 : 
c \leq \frac{f(n)}{g(n)} \leq d$$
Wie "eng" die Konstanten an den Term anliegen ist hierbei irrelevant. $0.00001 \leq c/d \leq 1000000$
meint immernoch dass es die selbe Wachstumsrate gibt.

$f(n)$ waechst **schneller** als $g(n)$, wenn es fuer alle positiven Konstanten $c$
ein $n_0$ gibt, ab dem $f(n) \geq c \cdot g(n)$ fuer alle $n \geq n_0$ gilt, d.h.: 
$$ \forall c \in \mathbb{R}_+ : \exists n_0 \in \mathbb{N} : \forall n \geq n_0 :
f(n) \geq c \cdot g(n)$$

or: $$\lim_{n\to\infty} \frac{g(n)}{f(n)} = 0$$

### Landau Symbole
Die sogenannten **Landau Symbole** sind eine Menge von Beschreibungen mit denen sich
Algorithmen gliedern lassen. Davon das wichtigste ist das Grosse-O, der rest sind
Varianten davon. 
$$O(f(n)) = \{g(n) : \exists c > 0 : \exists n_0 > 0 : \forall n \geq n_0 : g(n) \leq c \cdot f(n) \}$$
Prosaisch gelesen beinhalted $O$ alle Funktionen $g$ fuer die $f$ schneller waechst als $g$. 

Wenn man die Umgleichung umdreht, bekommt man alle Funktionen die schneller wachsen als $f$:
$$\Omega(f(n)) = \{g(n) : \exists c > 0 : \exists n_0 > 0 : \forall n \geq n_0 : g(n) \leq c \cdot f(n) \}$$

Noch etwas nuetzliches ist $\Theta$: 
$\Theta(f(n)) = O(f(n)) \cap \Omega(f(n))$

$\Theta$ ist eine scharfe Grenze nach oben und nach unten, waehrenddessen $O$ und $\Omega$
generelle Begrenzungen sind. So kann man sagen das eine Funktion $f(x) = n^2 - 5x +$ in $O(x^2)$
ist, aber ebenfalls auch in $O(x^3)$, $O(x!)$, etc, aber nur, als einziges, in $\Theta(x^2)$

Landau Symbole beschreiben **Mengen**, jedoch werden sie der Kuerze halber oft in "Gleichungen"
verwendet, z.B.: $g(n) = O(f(n))$, was strikt genommen keinen Sinn macht. Man muesste schreiben
$g(n) \in O(f(n))$. Deshalb sollte man solche Gleichungen immer nur **von links nach rechts** lesen. 

### Rechenregeln fuer Landau Symbole: 

#### Lemma
Sei $p$ in Polynom der Ordnung $k$ bzgl der Variable $n$, also
$$p(n) = \sum^k_{i=0}a_i \cdot n^i \text{  mit  } n_k > 0$$
Dann ist: $$p(n) \in \Theta(n^k)$$

#### Lemma
Fuer Funktionen $f(n)$ (und $g(n)$) mit $\exists n_0 : \forall n \geq n_0 : f(n) > 0$ gilt: 

- $c \cdot f(n) \in \Theta(f(n))$ fuer jede Konstante $c > 0$ 
- $O(f(n)) + O(g(n)) = O(f(n) + g(n))$
- $O(f(n)) \cdot O(g(n)) = O(f(n) \cdot g(n))$
- $O(f(n) + g(n)) = O(f(n))$ falls $g(n) \in O(f(n))$

Diese Ausdruecke sind auch korrekt fuer $\Omega$ statt $O$. Die letzte Regel heisst dann: 

- $\Omega(f(n) + g(n)) = \Omega(f(n))$ falls $g(n) \in O(f(n))$  
  Der Unterschied hier ist das $O$ im letzten Term, was bestehen bleibt.

### Ableitungen und $O$-Notation. 
Seien Funktionen $f$ und $g$ **differenzierbar**:

- falls $f'(n) \in O(g(n))$, dann auch $f(n) \in O(g(n))$
- falls $f'(n) \in \Omega(g(n))$, dann auch $f(n) \in \Omega(g(n))$
- falls $f'(n) \in o(g(n))$, dann auch $f(n) \in o(g(n))$
- falls $f'(n) \in \omega(g(n))$, dann auch $f(n) \in \omega(g(n))$

### Konkrete Laufzeitanalyse
Viele bei vielen Konstrukten kann man die Laufzeit aus dem code ablesen, zB: 
```python 
def signum(x):
  if x > 0:   # Auswertung boolean, O(1) 
    return 1  # ret: O(1)
  if x < 0    # O(1)
    return -1 # O(1)
  return 0    # O(1)
```
Dies ergibt als totale Laufzeit, natuerlich, $O(1)$. Ein etwas komplexeres Beispiel: 
```python
def sum_to(x): 
  ret = 0              # assignment, O(1)
  for i in range(0,x): # Loop, O(0 to n-1), plus O(1) fuer loop-condition auswertung.
    ret += i           # Assignment (1), addition O(1)

  return ret           # return, O(1)
```
Aufgrund der vorherigen Lemmas kann man kleine Terme rauswerfen, weshalb nur der loop
interessant ist. Wenn man potentielle Nebenkosten von `range` ignoriert, kommt man auf
$O(1+1+1\sum^{n-1}_{i=0} i)$, was sich schoen nach dem Polynom Lemma auf $O(n)$ reduziert,
was auch die totale Komplexitaet ist. 


# Overview, GAD 06/13
In this Lecture, we.... 

# Content
_Continuation of last lecture, about quicksort and its comparisons,
best/average/worst cases, et cetera._

## Lemma: 
Die Wahrscheinlichkeit, dass, in quicksort, zwei Elemente verglichen kann
beschrieben werden als $$Pr[X_{ij} =1 ] =\frac{2}{(j-i+1)}$$

## Verbessertes Quicksort: 

```java
void qSort(Element[] a, int l, int r) {
  while((r - l) >= n0) {
    j = pickPivotPos(a,l,r); 
    swap(a,l,j;
    p = a[l]; 
    int i = l; 
    int j = r; 
    do {
      while(a[i] < p) i++;
      while(a[j] > p) j--; 
      if (i <= j) {
        swap(a,i,j);
        i++;
        j--;
      }
    } while(i <= j)
    if(i < (l + r)/2) {
      qSort(a,l,r); 
      l = i;
    } else {
      qSort(a,i,r);
      r = j; 
    }
  }
  insertionSort(a,l,r); 
}
```

Sadly, this does not change anything about worst or average case, it's just a
little faster in practice. 

## Selektionsproblem.

- Bestimmug des kleinsten und groessten Elementes ist mit einem einzigen Scan
  ueber das Array in linearer Zeit moeglich. 
- Aber wie ist es beim _k-kleinsten_ Element, zB beim $n/2$-kleinsten Element,
  dem Median? 

## QuickSelect. 
Aehnlich wie Quicksort, aber nur eine Seite betrachten. Die Laufzeit von
QuickSelect ist $O(n)$. 

## Schnelleres Sortieren

- Mit paarweisen Vergleichen werden wir nie $O(n\log n)$ schlagen. 
- Was aber wenn es eine bessere Struktue hat? 
- Zb Nummerngvergleich koennen nicht gleich sein bei der ersten verschiedenen
  Zahl

Pseudo algorithm: 
```python
numbers = [0,1,2,3,4,5,6,7,8,9]
def ksort_numbers(elements): 
  buckets = {t: list() for t in numbers}
  for e in elements: 
    buckets[e[0]].append(e)

  l = list()
  [l.append(bucket) for bucket in buckets]
  ksort(l)
```

Plus some sort of stopping condition. This has a complexity of $\Theta(n+K)$,
wo K die Menge von Buckets ist. This means, when K is large, you have issues. 

- Interessant: K-sort ist **stabil**, d.h. gleich wertige elemente behalten
  ihre relative Reihenfolge. This is achieved by appending elements to the
  _end_ of the bucket they are inserted into.

## RadixSort. 

- Verwende _"K-adische Darstellung"_ der Schluessel. 
- Annahme: Schluessel sind Zahlen aus $\{0, \dots, K^d -1\}$, repraesentiert
  durch $d$ Stellen von Ziffern aus $\{0,\dots, K-1\}$
- Sortiere zunaechst entsprechend der niederwertigen Ziffer mit `kSort` und
  dann nacheinander fuer immer hoeherwertige Stellen
- behalte Ordnung der Teillisten bei. 

### Example Implementation
```java
void radixSort(Sequence<Element> s) { 
  for (int i = 0; i < d; i++) {
    kSort(s,i) // Sort according to key-i(x), where key-i
               // mit key-i(x) = (key(x)/K-i) % K
  }
}
```

Verfahren funktioniert, weil `kSort` stabil ist. Laufzeit der Radixsort ist
$O(d(n+K))$ fuer n Schluessel aus $\{0,\dots, K^d -1\}$ 


## Externes Sortieren. 

External sorting is what is required when you stop being able to fit the entire
set of data to sort into RAM. Then you have to deal with transactions between
thee Hauptspeicher, sized M, and the block size with which it transfers blocks,
M. You want to minimise the amount of block transfers. 

MergeSort is the primary idea here, because it works on subsequent blocks that
make for faster reading (sequential read) 

Eingabe: Grosses Feld auf der Festplatte
Annahme: Anzahl der Elemente n ist durch B (blocksize) Teilbar, sonst
Auffuellen. 

Run Formation Phase: 

- Lade wiederholt Teilfeld der Groesse M in RAM
- Sort with in-place algorithm 
- Push back sorted part to HDD. 
- This requires 2n/B transfers. 
- Results in sorted blocks, size M. 

Merge Phase: 
- Merge von jeweils 2 Teilfolgen in log2(n/m) Phasen
- Dabei jeweils Verdopplung der Groesse der sortieren Teile. 

Merging two blocks: 
- von jedem der beiden Runs und von der Ausgabe sequenz bleibt ein block im
  Hauptspeciher, (3 Puffer: 2x Eingabe, 1x Ausgabe)
- Am Anfang beide Inputbloecke mit einem Block laden, 
- merge in memory, write to block
- refill blocks as required
- write to disk when output block full. 

Pro Mergephase, the whole field is read and written. If you have more memory
than 3 blocks, just use more nad more input blocks and reduce time taken that
way. 

#6 Priority Queues

## 6.1 Allgemeines. 

- M: Menge von Elementen
- prio(e): Prioritaet von Element e.

Functions
```ruby
M.build(elements*) # => M([elements])
M.insert(e)        # => M([elements], e)
M.min()            # => Element with lowest priority.
M.deleteMin()      # => Removes and returns lowest-priority element 
```

Addressable priority queues allow for indexing and support more granular
access. Important for all this: Lower priority is better. 

Implementing this via exisiting structures like Lists is not really acceptable,
we need new datastructures. Answer: Heap. 

## 6.2 Heaps. 
- Idee: verwende Binaerbaum. 
- We want to uphold two invariants: 
    - Fast-vollstaendiger binaerbaum (no level except the lowest has less than
      two children)
    - The priority of the root of a given `Tree` element is always bigger, or
      equal to the one of its children. This means, the root is the
      lowest-priorty element. 

Binary trees are typically saved as binary tree, where the larger the index
goes, the higher the level of the child. A field also means no holes, which
means the first invariant is upheld. Accessing elements can be done well, and
easily via index arithmetics, meaning no expensive linked-lists, etc. 
  
  - The children of a node are at `field[2i+1]` and `field[2i+2]`. The father
    node is at `field[floor(i-1/2)]`. 

### Insert
- Form Invariante: `H[n] = e; SiftUp(n); n++;`
- Heap Invariante:  
  Swap e and its father node until `prio(H[floor((k-1)/2)]) <= prio(e)`. 

This has the fantastic complexity of $O(\log n)$

Revisit heaps. Fuck this, good lord

